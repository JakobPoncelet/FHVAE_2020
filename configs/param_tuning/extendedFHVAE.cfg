[RegFHVAE]

### Model
# options: LSTM or transformer
model = LSTM_bidirectional

# latent dimensions
z1_dim = 32
z2_dim = 32

### LSTM architecture
# encoder (hidden units/cells)
z1_rhus = [512, 512]
z2_rhus = [512, 512]
# decoder (hidden units/cells)
x_rhus = [512, 512]

### Transformer architecture
# model dimension
d_model = 256
# number of MHA layers in transformer encoder
num_enc_layers = 8
# number of transformer heads
num_heads = 8
# dimension of inner feedforward layer
dff = 512
# maximum length of positional encoding
pe_max_len = 8000
# dropout rate
rate = 0.1

### Loss factors
# discriminative objective weight
bump_logpmu1 = 1000.0
alpha_dis_z1 = 10.0
alpha_dis_z2 = 10.0
# regularization weight on z1
alpha_reg_b = 0.0
# regularization weight on z2
alpha_reg_c = 0.0
# adversarial regularization weights on z1/z2
alpha_advreg_b = 0.0
alpha_advreg_c = 0.0
# priors: [pz1_stddev, pmu1_stddev, pz2_stddev, pmu2_stddev]
priors = [0.5, 1.0, 0.5, 1.0]
# remove noise from z1 (clean-noisy similarity), DEPRECATED
alpha_noise = 0.0

## householder flow
num_flow_steps = 0

### Training settings
# batch size
batch_size = 500
# number of sequences for hierarchical sampling (2000 for timit, 5000 others)
nmu2 = 5000
# number of maximum training epochs
n_epochs = 150
# number of maximum consecutive non-improving epochs
n_patience = 20

### Optimizer settings
# learning rate, can be fixed e.g. '0.001' or 'custom' (for transformer model)
lr = 0.001
# number of warm up steps when using custom learning rate (lr will steeply rise until step=warmup, and slowly degrade afterwards)
warmup_steps = 4000
# tunable scalar that multiplies the custom learning rate formula
k = 20
# epsilon parameter of adam
adam_eps = 1.0e-3
# memory factors
beta1 = 0.95
beta2 = 0.999

## Finetuning multi-condition stage
finetuning = false
# number of noisy versions of every clean speech file if you want explicit clean<->noise mapping during training
num_noisy_versions = 4
n_ft_epochs = 0
# SGD optimizer settings
lr_finetune = 0.0001
momentum = 0.9
# use nesterov momentum
nesterov = true
# gradient accumulation of multiple batches
accum_num_batches = 5

### Regularizing training factors
# list of regularizing training factors, e.g. gender:region:spk
facs = gender:reg1
# list of regularizing training factors that are time aligned, e.g. phones:class1:class2
talabs = phones
# locations of factor files
fac_root = /esat/spchdisk/scratch/jponcele/fhvae_jakob/datasets/cgn_allcomps_and_telephone/fac/all_facs_%s.scp

### Data locations
# which dataset to use for training
dataset = cgn_allcomps_and_telephone
# where the dataset is stored, one higher than dataset (default is ./datasets)
datadir = /esat/spchdisk/scratch/jponcele/fhvae_jakob/datasets

### Configurations for testing
# name of dataset to evaluate on (testing on different dataset is still experimental!!)
dataset_test = cgn_allcomps_and_telephone
# name of dataset partition to evaluate, e.g. dev or test
set_name = test
